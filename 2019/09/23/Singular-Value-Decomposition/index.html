<!DOCTYPE html>
<html>
<head>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><meta charset="utf-8">
  

  
  <title>数学：SVD分解[1]——原理 | Hako</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Preface 前言SVD (Singular Value Decomposition) 在机器学习等其他领域有着非常广泛的应用，本文主要是受到计算机图像学课和以前看到的材料的启发，希望对SVD有更深入的了解。 相关的pdf资料可以参见我github上的记录。本文行文基本上是对它的翻译和理解。">
<meta property="og:type" content="article">
<meta property="og:title" content="数学：SVD分解[1]——原理">
<meta property="og:url" content="mercurixito.github.io/2019/09/23/Singular-Value-Decomposition/index.html">
<meta property="og:site_name" content="Hako">
<meta property="og:description" content="Preface 前言SVD (Singular Value Decomposition) 在机器学习等其他领域有着非常广泛的应用，本文主要是受到计算机图像学课和以前看到的材料的启发，希望对SVD有更深入的了解。 相关的pdf资料可以参见我github上的记录。本文行文基本上是对它的翻译和理解。">
<meta property="og:locale" content="zh_CN">
<meta property="og:image" content="/images/2019-09-23/unprocessed.png">
<meta property="og:image" content="/images/2019-09-23/processed2.png">
<meta property="og:image" content="/images/2019-09-23/processed1.png">
<meta property="article:published_time" content="2019-09-23T08:05:33.000Z">
<meta property="article:modified_time" content="2020-03-17T08:28:08.809Z">
<meta property="article:author" content="Victor Chen">
<meta property="article:tag" content="Mathematic">
<meta property="article:tag" content="Linear Algebra">
<meta name="twitter:card" content="summary">
<meta name="twitter:image" content="/images/2019-09-23/unprocessed.png">
  
    <link rel="alternate" href="/atom.xml" title="Hako" type="application/atom+xml">
  
  
    <link rel="icon" href="/favicon.png">
  
  
    <link href="//fonts.googleapis.com/css?family=Source+Code+Pro" rel="stylesheet" type="text/css">
  
  
<link rel="stylesheet" href="/css/style.css">

<meta name="generator" content="Hexo 4.2.0"><!-- hexo-inject:begin --><!-- hexo-inject:end --></head>

<body>
  <!-- hexo-inject:begin --><!-- hexo-inject:end --><div id="container">
    <div id="wrap">
      <header id="header">
  <div id="banner"></div>
  <div id="header-outer" class="outer">
    <div id="header-title" class="inner">
      <h1 id="logo-wrap">
        <a href="/" id="logo">Hako</a>
      </h1>
      
    </div>
    <div id="header-inner" class="inner">
      <nav id="main-nav">
        <a id="main-nav-toggle" class="nav-icon"></a>
        
          <a class="main-nav-link" href="/">Home</a>
        
          <a class="main-nav-link" href="/archives">Archives</a>
        
      </nav>
      <nav id="sub-nav">
        
          <a id="nav-rss-link" class="nav-icon" href="/atom.xml" title="RSS Feed"></a>
        
        <a id="nav-search-btn" class="nav-icon" title="搜索"></a>
      </nav>
      <div id="search-form-wrap">
        <form action="//google.com/search" method="get" accept-charset="UTF-8" class="search-form"><input type="search" name="q" class="search-form-input" placeholder="Search"><button type="submit" class="search-form-submit">&#xF002;</button><input type="hidden" name="sitesearch" value="mercurixito.github.io"></form>
      </div>
    </div>
  </div>
</header>
      <div class="outer">
        <section id="main"><article id="post-Singular-Value-Decomposition" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-meta">
    <a href="/2019/09/23/Singular-Value-Decomposition/" class="article-date">
  <time datetime="2019-09-23T08:05:33.000Z" itemprop="datePublished">2019-09-23</time>
</a>
    
  </div>
  <div class="article-inner">
    
    
      <header class="article-header">
        
  
    <h1 class="article-title" itemprop="name">
      数学：SVD分解[1]——原理
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="Preface-前言"><a href="#Preface-前言" class="headerlink" title="Preface 前言"></a>Preface 前言</h2><p><code>SVD (Singular Value Decomposition)</code> 在机器学习等其他领域有着非常广泛的应用，本文主要是受到计算机图像学课和以前看到的材料的启发，希望对<code>SVD</code>有更深入的了解。</p>
<p>相关的pdf资料可以参见<a href="https://github.com/MercuriXito/BUAA-Gradudate-Lessons/blob/master/2019%20autumn/computer%20graphic/lesson%202/2018_Lecture02-SVD.pdf" target="_blank" rel="noopener">我<code>github</code>上的记录</a>。本文行文基本上是对它的翻译和理解。</p>
<a id="more"></a>

<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><p>我们先叙述一些名词和涉及到的线性代数中的一些看法。</p>
<p>非齐次线性方程组${Ax=b}$的计算可以有两种形式，这取决于我们对矩阵具体含义的看法。</p>
<p>如计算：</p>
<p>$${<br>  \left[<br>    \begin{matrix} 1 &amp; 3 \\ -3 &amp; 2 \\ \end{matrix}<br>  \right] \times<br>  \left[<br>    \begin{matrix} 3 \\ 1 \end{matrix}<br>  \right]<br>}$$</p>
<p>我们一般习惯的计算方式是<code>row-way</code>：${a_{i,j}}$是左矩阵的${i}$行和右矩阵的${j}$列对应相乘求和，即：</p>
<p>$${<br>  \left[<br>    \begin{matrix} 1 &amp; 3 \\ -3 &amp; 2 \\ \end{matrix}<br>  \right] \times<br>  \left[<br>    \begin{matrix} 3 \\ 1 \end{matrix}<br>  \right] =<br>  \left[<br>    \begin{matrix}<br>    \left[ \begin{matrix} 1 &amp; 3 \end{matrix} \right] \times \left[ \begin{matrix} 3 \\ 1 \end{matrix} \right] \\<br>    \left[ \begin{matrix} -3 &amp; 2 \end{matrix} \right] \times \left[ \begin{matrix} 3 \\ 1 \end{matrix} \right]<br>    \end{matrix}<br>  \right] =<br>  \left[ \begin{matrix} 6 \\ 7 \end{matrix} \right]<br>}$$</p>
<p><code>column-way</code>将矩阵看成列向量组，我们得到的结果${b}$就是列向量组的线性组合，系数为${x}$。</p>
<p>$${<br>  \left[<br>    \begin{matrix} 1 &amp; 3 \\ -3 &amp; 2 \\ \end{matrix}<br>  \right] \times<br>  \left[<br>    \begin{matrix} 3 \\ 1 \end{matrix}<br>  \right] =<br>  3  \times \left[ \begin{matrix} 1 \\ -3 \end{matrix} \right] +<br>  1 \times \left[ \begin{matrix} 3 \\ 1 \end{matrix} \right] =<br>  \left[ \begin{matrix} 6 \\ 7 \end{matrix} \right]<br>}$$</p>
<p><code>Column-Way</code>使得我们将求解非齐次线性方程组${Ax=b}$看成用列向量组${A}$表出${b}$，如果：</p>
<ul>
<li>${b}$在列向量组张成的空间中，则我们一定可以求得${x}$，它是表出的系数。  </li>
<li>${b}$不在列向量组张成的空间中，我们希望求得它的最小二乘解。  </li>
</ul>
<h2 id="Anotations"><a href="#Anotations" class="headerlink" title="Anotations"></a>Anotations</h2><p>我们给出四个特殊的名词。</p>
<h3 id="Perpframes-正交标架"><a href="#Perpframes-正交标架" class="headerlink" title="Perpframes(正交标架)"></a>Perpframes(正交标架)</h3><hr>
<p><code>Perpframe</code>一词源于<code>Perpendicular</code>(垂直)和<code>frame</code>(框架)。更好的理解的说法是<code>orthonormal basis</code>(单位正交基)。我们给定一组单位正交基${\left[v_{0},v_{1}, \cdots, v_{n}\right]}$，它满足:</p>
<p>$${<br>  \left\{<br>    \begin{array}{lc}<br>    v_{i}^{T}v_{j} = 0 &amp; i \neq j \\<br>    v_{i}^{T}v_{j} = 1 &amp; i = j<br>    \end{array}<br>  \right.<br>}$$</p>
<p>以下我们使用一组二维平面的单位正交基${[v_{1},v_{2}]}$来叙述。</p>
<h3 id="Aligners"><a href="#Aligners" class="headerlink" title="Aligners"></a>Aligners</h3><hr>
<p>上述的单位正交基对应的Aligners 定义为: </p>
<p>$${\left[ \begin{matrix} v_{1}^{T} \\ v_{2}^{T} \end{matrix} \right]}$$</p>
<p>它具有如下的性质：</p>
<p>$${<br>\left[ \begin{matrix} v_{1}^{T} \\ v_{2}^{T} \end{matrix} \right] \cdot<br>v_{1} =<br>\left[ \begin{matrix} v_{1}^{T} \cdot v_{1} \\ v_{2}^{T} \cdot v_{1} \end{matrix} \right] =<br>\left[ \begin{matrix} 1 \\ 0 \end{matrix} \right] \quad<br>\left[ \begin{matrix} v_{1}^{T} \\ v_{2}^{T} \end{matrix} \right] \cdot<br>v_{2} =<br>\left[ \begin{matrix} v_{1}^{T} \cdot v_{2} \\ v_{2}^{T} \cdot v_{2} \end{matrix} \right] =<br>\left[ \begin{matrix} 0 \\ 1 \end{matrix} \right]<br>}$$</p>
<p>也就是说，Aligners将单位正交基投影到x-y坐标轴(即二维平面的标准正交基${\left[e_{1},e_{2} \right]}$)。</p>
<h3 id="Hangers"><a href="#Hangers" class="headerlink" title="Hangers"></a>Hangers</h3><hr>
<p>上述的单位正交基对应的Hangers定义为：</p>
<p>$${\left[ v_{1}, v_{2} \right]}$$</p>
<p>它具有的性质是：</p>
<p>$${<br>  \left[ v_{1}, v_{2} \right] \cdot \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right] = v_{2} \quad<br>  \left[ v_{1}, v_{2} \right] \cdot \left[ \begin{matrix} 1 \\ 0 \end{matrix} \right] = v_{1} \quad<br>}$$</p>
<p>也就是说，Hangers的作用正好和Aligners相反，它把标准正交基投影到我们的单位正交基上。</p>
<p>同时我们注意到：</p>
<p>$${<br>  Aligners \cdot Hangers =<br>  \left[ \begin{matrix} v_{1}^{T} \\ v_{2}^{T} \end{matrix} \right] \cdot<br>  \left[ \begin{matrix} v_{1} &amp; v_{2} \end{matrix} \right] =<br>  \left[ \begin{matrix} 1 &amp; 0 \\ 0 &amp; 1 \end{matrix} \right] = E<br>}$$</p>
<p>这说明：同一组单位正交基的Aligners和Hangers是互逆的。</p>
<h3 id="Stretchers-伸缩"><a href="#Stretchers-伸缩" class="headerlink" title="Stretchers(伸缩)"></a>Stretchers(伸缩)</h3><hr>
<p>Stretchers比较好理解，它是对角阵：</p>
<p>$${<br>  Stretchers = diagnal(\lambda_{0}, \lambda_{1}, \cdots, \lambda_{n})<br>}$$</p>
<p>如${\textbf{R}^{2}}$上的一个Stretchers定义为:</p>
<p>$${\left[ \begin{matrix} a &amp; 0 \\ 0 &amp; b \end{matrix} \right]}$$</p>
<p>对任意二位平面内的向量${\left[  x,y \right]^{T}}$，Stretchers对它进行变换有：</p>
<p>$${<br>  \left[ \begin{matrix} a &amp; 0 \\ 0 &amp; b \end{matrix} \right] \cdot<br>  \left[ \begin{matrix} x \\ y \end{matrix} \right] =<br>  \left[ \begin{matrix} ax \\ by \end{matrix} \right] =<br>  x \cdot \left[ \begin{matrix} a \\ 0 \end{matrix} \right] +<br>  y \cdot \left[ \begin{matrix} 0 \\ b \end{matrix} \right]<br>}$$</p>
<p>$${<br>  \left[ \begin{matrix} a \\ 0 \end{matrix} \right] = a \cdot \left[ \begin{matrix} 1 \\ 0 \end{matrix} \right] = a \cdot e_{1} \quad<br>  \left[ \begin{matrix} 0 \\ b \end{matrix} \right] = b \cdot \left[ \begin{matrix} 0 \\ 1 \end{matrix} \right] = b \cdot e_{2}<br>}$$</p>
<p>即Stretchers对应位置上的值对对应维度上的单位正交基进行了放缩。</p>
<h3 id="Coordinate-坐标"><a href="#Coordinate-坐标" class="headerlink" title="Coordinate(坐标)"></a>Coordinate(坐标)</h3><hr>
<p>对任意二维平面上的向量${p=\left[  x_{1}, x_{2} \right]^{T}}$，首先我们要理解这里给出的坐标实际是${p}$在标准正交基下的坐标，如果我们想求它在单位正交基${\left[  v_{1},v_{2} \right]}$下的坐标：</p>
<p>$${<br>  p = av_{1} + bv_{2}<br>  = \left[ v_{1}, v_{2} \right] \cdot<br>  \left[ \begin{matrix} a \\ b \end{matrix} \right]<br>  = \left[  e_{1}, e_{2} \right] \cdot<br>  \left[ \begin{matrix} x_{1} \\ x_{2} \end{matrix} \right]<br>}$$</p>
<p>我们用Aligners左乘上面的式子得到：</p>
<p>$${<br>  Aligners \cdot p<br>  = \left[ \begin{matrix} v_{1}^{T} \\ v_{2}^{T}  \end{matrix} \right] \left[ v_{1}, v_{2} \right] \cdot<br>  \left[ \begin{matrix} a \\ b \end{matrix} \right]<br>  = \left[ \begin{matrix} a \\ b \end{matrix} \right]<br>}$$</p>
<p>上面的式子里${\left[ v_{1}, v_{2} \right] = Hangers}$，则：</p>
<p>$${<br>  \left[ v_{1}, v_{2} \right] \cdot<br>  \left[ \begin{matrix} a \\ b \end{matrix} \right]<br>  = \left[ \begin{matrix} x_{1} \\ x_{2} \end{matrix} \right] = p<br>}$$</p>
<p>我们从以上可以看出：  </p>
<ul>
<li>Aligners将向量在标准正交基上的坐标变换到它的单位正交基上的坐标。</li>
<li>Hangers将向量在它的单位正交基上的坐标变换到标准正交基上的坐标。</li>
</ul>
<h3 id="Example-例子"><a href="#Example-例子" class="headerlink" title="Example (例子)"></a>Example (例子)</h3><hr>
<p>给定二维平面上的一组单位正交基:</p>
<p>$${<br>  \left\{<br>    \begin{array}{l}<br>      v_{1} = \left[  \frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2} \right]^T \\<br>      v_{2} = \left[  -\frac{\sqrt{2}}{2}, \frac{\sqrt{2}}{2} \right]^T<br>    \end{array}<br>  \right.<br>}$$</p>
<p>求向量${p=\left[ 3,5 \right]^{T}}$到${v_{1}}$的垂足。</p>
<p><strong>解</strong>：<br>${p}$在x-y轴上对于x轴上的垂足我们很容易得到是${\left[ 3,0 \right]}$，我们只需要令y轴的坐标为0，也就是说我们将一个Stretchers作用于${p}$：</p>
<p>$${<br>  \left[ \begin{matrix} 1 &amp; 0 \\ 0 &amp; 0 \end{matrix} \right] \cdot p = \left[ \begin{matrix} 3 \\ 0 \end{matrix} \right]<br>}$$</p>
<p>这个Stretchers保持${p}$在x轴的伸缩尺度为1，在y轴的伸缩尺度为0。</p>
<p>那我们的思路可以是：将${p}$变换到给定的单位正交基上，用该Stretchers作用它，得到垂足在单位正交基上的坐标，再将其变换回x-y坐标系，一系列的变换矩阵${A}$为：</p>
<p>$${<br>  A \cdot p = (Hangers) \cdot (Stretchers) \cdot (Aligners) \cdot p =<br>  \left[ \begin{matrix} \frac{\sqrt{2}}{2} &amp; - \frac{\sqrt{2}}{2} \\ \frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \end{matrix} \right] \cdot \left[ \begin{matrix} 1 &amp; 0 \\ 0 &amp; 0  \end{matrix}  \right] \cdot \left[ \begin{matrix} \frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \\ -\frac{\sqrt{2}}{2} &amp; \frac{\sqrt{2}}{2} \end{matrix} \right] \cdot \left[ \begin{matrix} 3 \\ 5 \end{matrix} \right] = \left[ \begin{matrix} 4 \\ 4 \end{matrix} \right]<br>}$$</p>
<p>这就是整个的变换过程，也是计算机图像学中几个变换矩阵连续变换的理论，将复杂的变换分解有利于计算。</p>
<h2 id="Singular-Value-Decomposition-SVD分解"><a href="#Singular-Value-Decomposition-SVD分解" class="headerlink" title="Singular Value Decomposition (SVD分解)"></a>Singular Value Decomposition (SVD分解)</h2><p>SVD分解就是这样一个命题：对任意一个${m \times n}$的矩阵${A}$，存在以下的分解形式：</p>
<p>$${A  = (Hangers) \cdot (Stretchers) \cdot (Aligners) }$$</p>
<p><strong>SVD的精妙之处在于：它对任意的矩阵都成立，任意的矩阵都有如上的分解形式。</strong></p>
<p>问题是我们<strong>如何找到这样的${(Hangers) (Stretchers) (Aligners)}$</strong>。</p>
<h3 id="构造"><a href="#构造" class="headerlink" title="构造"></a>构造</h3><p>首先，对任意一个${m \times n}$的矩阵${A}$，我们可以将它看成是${\textbf{R}^{n} \rightarrow \textbf{R}^{m}}$的变换(线性映射)。我们可以在${\textbf{R}^{n}}$里找到一组单位正交基${\left[  a_{1}, a_{2}, \cdots, a_{n} \right]}$，而且这样的单位正交基有任意多个。</p>
<p>我们将这组单位正交基通过${A}$投影到另一组向量${\left[  d_{1}, d_{2}, \cdots, d_{n} \right]}$(<strong>注意该向量组共有n个元素，每个元素是m维</strong>)。</p>
<p>我们定义标量${s_{i} = \vert\vert d_{i} \vert\vert = \vert\vert Aa_{i} \vert\vert}$。</p>
<p>由此我们定义另一组向量 ${\left[ h_{1}, h_{2}, \cdots, h_{n}\right]}$，它们满足：</p>
<p>$${<br>  h_{i} =<br>  \left\{<br>    \begin{array}{cl}<br>      \vec{0} &amp; if \quad s_{i} = 0 \\<br>      \frac{1}{s_{i}} A a_{i} &amp; if \quad s_{i} \neq 0<br>    \end{array}<br>  \right.<br>}$$</p>
<p>我们可以发现以下的式子成立：</p>
<p>$${<br>\begin{aligned}<br>  H \cdot S \cdot A’ &amp;=<br>  \left[ h_{1}, h_{2}, \cdots, h_{n}\right] \cdot<br>  \left[ \begin{matrix}<br>   s_{1} &amp; &amp;  &amp;  \\<br>   &amp; s_{2} &amp; &amp; \\<br>   &amp; &amp; \ddots &amp; \\<br>   &amp; &amp;  &amp; s_{n} \\<br>   \end{matrix} \right] \cdot<br>   \left[ \begin{matrix} a_{1}^{T} \\ \vdots \\ a_{n}^{T} \end{matrix} \right] \\<br>    &amp; =<br>   \left[ Aa_{1}, Aa_{2}, \cdots, Aa_{n}\right] \cdot \left[ \begin{matrix} a_{1}^{T} \\ \vdots \\ a_{n}^{T} \end{matrix} \right] \\<br>    &amp; = A \left[ a_{1}, a_{2} \cdots, a_{n} \right] \left[ \begin{matrix} a_{1}^{T} \\ a_{2}^{T} \\ \vdots \\ a_{n}^{T} \end{matrix} \right] \\<br>    &amp; = A<br>\end{aligned}<br>}$$</p>
<p>特别注意的是以上的<strong>矩阵的大小</strong>：</p>
<p>${H}$的大小为${m \times n}$，${S}$的大小为${n \times n}$，是一个对角阵，同时是一个${Stretchers}$；${A’}$的大小为${n \times n}$，同时是一个${Aligner}$。只有${H}$不一定是${Hanger}$。</p>
<p>基于以上的构造方法，我们发现现在的问题是：寻找合适的${\textbf{R}^{n}}$空间的单位正交基${\left[  a_{1}, a_{2}, \cdots, a_{n} \right]}$使得通过以上构造方法得到的${H}$为${Hanger}$，那我们就得到了对一个矩阵进行SVD分解的三个矩阵，同时证明了任意一个矩阵有SVD分解。</p>
<h3 id="构造Hanger"><a href="#构造Hanger" class="headerlink" title="构造Hanger"></a>构造Hanger</h3><h4 id="得到单位正交向量组"><a href="#得到单位正交向量组" class="headerlink" title="得到单位正交向量组"></a>得到单位正交向量组</h4><p>要使得向量组${\left[  h_{1}, h_{2}, \cdots, h_{n} \right]}$构成${Hanger}$，当且仅当该向量组也是一组单位正交基。</p>
<p>对任意${h_{i}}$，根据之前的${s_{i}}$的定义，只要${s_{i}\neq 0}$，我们有：</p>
<p>$${<br>  h_{i} \cdot h_{i} = \frac{1}{s_{i}^2} || Aa_{i} ||^{2} = 1<br>}$$</p>
<p>剩下的关键就是如何使得${h_{i}}$正交，即</p>
<p>$${<br>  h_{i} \cdot h_{j} = 0 \quad if \quad (i \neq j)<br>}$$</p>
<p>我们如下推导：</p>
<p>$${<br>\begin{aligned}<br>    h_{i} \cdot h_{j} &amp;= Aa_{i} \cdot Aa_{j} &amp;= (Aa_{i})^{T}Aa_{j} &amp;= 0 \\<br>    &amp;= a_{i}^{T} (A^{T} Aa_{j})<br>\end{aligned}<br>}$$</p>
<p>我们只要令:</p>
<p>$${<br>  A^{T}Aa_{j} = \lambda a_{j}<br>}$$</p>
<p>就可以使得:</p>
<p>$${a_{i}^{T}(A^{T} Aa_{j}) = a_{i}^{T}\lambda a_{j} = 0 = h_{i}h_{j} \quad if \quad (i \neq j)}$$</p>
<p>通过上面的推导，我们发现这唯一的一组正交基正是${A^{T}A}$的特征向量${\left[  a_{1}, a_{2}, \cdots, a_{n} \right]}$。而且由于${A^{T}A}$是实对称阵，${\left[  a_{1}, a_{2}, \cdots, a_{n} \right]}$一定存在，通过它得到的向量组${\left[  h_{1}, h_{2}, \cdots, h_{n} \right]}$一定是正交的。</p>
<p>这里我们还有最后一个问题，向量组${\left[  h_{1}, h_{2}, \cdots, h_{n} \right]}$<strong>是否是${\textbf{R}^{m}}$的基</strong>。</p>
<h4 id="基的讨论"><a href="#基的讨论" class="headerlink" title="基的讨论"></a>基的讨论</h4><p>首先，由矩阵秩的不等式：</p>
<p>$${rank(A^{T}A) \leq min(rank(A^T), rank(A)) = rank(A) \leq min(m,n)}$$</p>
<p>如果${m \neq n}$，矩阵${A^{T}A}$一定不是满秩，则它一定存在${\lambda = 0}$的特征值，且由于矩阵${A^{T}A}$可分解，它的特征值的几何重数等于代数重数，该特征值对应的特征向量的个数等于${n-rank(A^{T}A)}$。我们把这些特征向量记作${c_{i}}$，其中${i=1,\cdots,n-rank(A^{T}A)}$。</p>
<p>对${c_{i}}$，我们有：</p>
<p>$${A^{T}Ac_{i}=0}$$</p>
<p>则</p>
<p>$${c_{i}^{T}A^{T}Ac_{i} = 0 = ||Ac_{i}||}$$</p>
<p>又根据${s_{i}}$的定义，我们知道这些${c_{i}}$对应的${h_{i}=Ac_{i}}$都是零向量。也就是说向量组${\left[  h_{1},h_{2},\cdots,h_{n} \right]}$中必定存在至少${n-rank(A^{T}A)}$个零向量。</p>
<p>又</p>
<p>$${n-rank(A^{T}A) \geq n-rank(A) \geq n - min(m,n)}$$</p>
<p>我们下面分析${m,n}$的大小的两种情况：</p>
<ul>
<li><p>${m &lt; n}$</p>
<p>此时矩阵${A}$就是高维空间向低维空间的映射，它完成了<strong>降维</strong>的功能。</p>
<p>如果${m &lt; n}$，那么向量组${\left[  h_{1},h_{2},\cdots,h_{n} \right]}$中必定存在至少${n-m}$个零向量，我们把这些零向量去除，得到至多包含${m}$个向量的向量组${\left[  h_{1}, h_{2}, \cdots, h_{m} \right]}$。</p>
<p>如果${A}$的秩为m，那么向量组${\left[  h_{1}, h_{2}, \cdots, h_{m} \right]}$中就不再包含零向量。该向量组包含m个向量，每个向量又是m维的，那么该向量组一定是${\textbf{R}^{m}}$的基。</p>
<p>如果${A}$的秩小于m，那么向量组${\left[  h_{1}, h_{2}, \cdots, h_{m} \right]}$中仍有零向量，我们仍将他们去除，变成${\left[  h_{1}, h_{2}, \cdots, h_{k} \right]}$，我们将该向量组扩充，向该向量组添加${m-k}$个互相正交的向量，得到的新的向量组${\left[  h_{1}, h_{2}, \cdots, h_{m} \right]}$就构成${\textbf{R}^{m}}$的基。</p>
</li>
<li><p>${m \geq n}$</p>
<p>此时矩阵${A}$是<strong>升维</strong>功能。</p>
<p>此时的分析过程大致和上面的情况相同，向量组${\left[  h_{1},h_{2},\cdots,h_{n} \right]}$中必定包含至少0个零向量，如果${A}$的秩小于n，我们仍然将对应的零向量去除。无论是否去除零向量，如果向量组中向量的个数小于m，我们仍添加正交的向量使整个向量组包含m个正交向量，这样得到${\textbf{R}^{m}}$的基。</p>
</li>
</ul>
<p>总结一下就是：<strong>使用之前的构造方法必然得到至多${m}$个正交向量${h_{i}}$，如果向量组个数小于${m}$，我们就扩充该向量组，这样必然可以得到${\textbf{R}^{m}}$的基。</strong></p>
<p>_<strong>而且，因为这些扩充的向量对应的${s_{i}}$是0，所以这样的${H}$和${S,A’}$相乘时仍然可以还原成${A}$。</strong>_</p>
<p>这里还有一个比较值得我们注意的是<strong>在SVD分解中三个矩阵的大小</strong>，在我们最初的构造过程中我们得到的分解模式</p>
<p>$${A = H \cdot S \cdot A’}$$</p>
<p>${H}$的大小为${m \times n}$，${S}$的大小为${n \times n}$，${A’}$的大小是${n \times n}$。</p>
<p>如果我们去掉${H}$列向量组中的零向量，对应的我们去除${S}$中对应的行，我们得到的矩阵大小现在是：${H}$的大小为${m \times k}$，${S}$的大小为${k \times n}$。</p>
<p>我们按照前述的得到${\textbf{R}^{m}}$的基的方法，如果${k &lt; m}$就添加正交向量将${H}$列向量组扩充为m个，并在${S}$的相应位置添加全零行。我们现在得到的矩阵大小是：${H}$的大小为${m \times m}$，${S}$的大小为${m \times n}$，${A’}$的大小是${n \times n}$。这就得到了我们想要的满足SVD定义的标准分解。</p>
<h3 id="关于SVD最后想说的几点"><a href="#关于SVD最后想说的几点" class="headerlink" title="关于SVD最后想说的几点"></a>关于SVD最后想说的几点</h3><ul>
<li><p>${Stretcher}$矩阵的大小和矩阵${A}$大小相等。</p>
<blockquote>
<p>The dimensions of the stretcher matrix will always match the dimensions of A.  </p>
</blockquote>
</li>
<li><p><strong>对应关系</strong>：${Hanger}$矩阵的第${\textbf{i}}$列，对应${Stretcher}$矩阵的第${\textbf{i}}$行。${Stretcher}$矩阵的第${\textbf{i}}$列，对应${Aligner}$矩阵的第${\textbf{i}}$行。（废话）也就是如果要删去行列，要遵守这一规则，这将在PCA中有用。</p>
</li>
</ul>
<h2 id="Application-of-SVD"><a href="#Application-of-SVD" class="headerlink" title="Application of SVD"></a>Application of SVD</h2><p>限于篇幅，有关于SVD分解的应用将在另外的文章中叙述。</p>
<p>比如下面是一个SVD分解的例子，任意给定一个${3 \times 3}$的矩阵，它将三维空间的样本变换到相同的三维空间上。</p>
<p><img src="/images/2019-09-23/unprocessed.png" alt="任意三维空间的变换"></p>
<p>但是如果我们对它进行SVD分解，再去掉相应的维度的变换，就可以降低该矩阵的维度，使他将样本变换到二维空间，甚至是一维空间，这就是<strong>矩阵降秩(或者说矩阵低秩近似)问题</strong>。</p>
<p>以下就是将该矩阵降秩到二和一的情况。</p>
<p><img src="/images/2019-09-23/processed2.png" alt="降秩到二"></p>
<p><img src="/images/2019-09-23/processed1.png" alt="降秩到一"></p>

      
    </div>
    <footer class="article-footer">
      <a data-url="mercurixito.github.io/2019/09/23/Singular-Value-Decomposition/" data-id="ck7ygs2jr0008bgucgtv40a6h" class="article-share-link">Share</a>
      
      
  <ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Linear-Algebra/" rel="tag">Linear Algebra</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/Mathematic/" rel="tag">Mathematic</a></li></ul>

    </footer>
  </div>
  
    
<nav id="article-nav">
  
    <a href="/2019/10/14/KL-Divergenece1/" id="article-nav-newer" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Newer</strong>
      <div class="article-nav-title">
        
          数学:KL散度
        
      </div>
    </a>
  
  
    <a href="/2019/05/26/RP-DNN-acc1/" id="article-nav-older" class="article-nav-link-wrap">
      <strong class="article-nav-caption">Older</strong>
      <div class="article-nav-title">论文研读(1.1)：超深层网络加速</div>
    </a>
  
</nav>

  
</article>

</section>
        
          <aside id="sidebar">
  
    

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签</h3>
    <div class="widget">
      <ul class="tag-list" itemprop="keywords"><li class="tag-list-item"><a class="tag-list-link" href="/tags/GAN/" rel="tag">GAN</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Linear-Algebra/" rel="tag">Linear Algebra</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Mathematic/" rel="tag">Mathematic</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Paper-Reading/" rel="tag">Paper Reading</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/Probability/" rel="tag">Probability</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/metrics/" rel="tag">metrics</a></li><li class="tag-list-item"><a class="tag-list-link" href="/tags/%E6%9D%82%E8%B0%88/" rel="tag">杂谈</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">标签云</h3>
    <div class="widget tagcloud">
      <a href="/tags/GAN/" style="font-size: 20px;">GAN</a> <a href="/tags/Linear-Algebra/" style="font-size: 10px;">Linear Algebra</a> <a href="/tags/Mathematic/" style="font-size: 15px;">Mathematic</a> <a href="/tags/Paper-Reading/" style="font-size: 15px;">Paper Reading</a> <a href="/tags/Probability/" style="font-size: 10px;">Probability</a> <a href="/tags/metrics/" style="font-size: 15px;">metrics</a> <a href="/tags/%E6%9D%82%E8%B0%88/" style="font-size: 10px;">杂谈</a>
    </div>
  </div>

  
    
  <div class="widget-wrap">
    <h3 class="widget-title">归档</h3>
    <div class="widget">
      <ul class="archive-list"><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/03/">三月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2020/02/">二月 2020</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/10/">十月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/09/">九月 2019</a></li><li class="archive-list-item"><a class="archive-list-link" href="/archives/2019/05/">五月 2019</a></li></ul>
    </div>
  </div>


  
    
  <div class="widget-wrap">
    <h3 class="widget-title">最新文章</h3>
    <div class="widget">
      <ul>
        
          <li>
            <a href="/2020/03/20/GAN-metrics2/">GAN metrics (2)</a>
          </li>
        
          <li>
            <a href="/2020/03/19/GAN-metrics1/">GAN metrics (1)</a>
          </li>
        
          <li>
            <a href="/2020/03/16/first/">First Post</a>
          </li>
        
          <li>
            <a href="/2020/02/07/RP-WGAN/">论文研读：WGAN</a>
          </li>
        
          <li>
            <a href="/2019/10/14/KL-Divergenece1/">数学:KL散度</a>
          </li>
        
      </ul>
    </div>
  </div>

  
</aside>
        
      </div>
      <footer id="footer">
  
  <div class="outer">
    <div id="footer-info" class="inner">
      &copy; 2020 Victor Chen<br>
      Powered by <a href="http://hexo.io/" target="_blank">Hexo</a><br>
      banner image from <a href="https://www.pixiv.net/artworks/80036479" target="_blank">This</a>
    </div>
  </div>
</footer>
    </div>
    <nav id="mobile-nav">
  
    <a href="/" class="mobile-nav-link">Home</a>
  
    <a href="/archives" class="mobile-nav-link">Archives</a>
  
</nav>
    

<script src="//ajax.googleapis.com/ajax/libs/jquery/2.0.3/jquery.min.js"></script>


  
<link rel="stylesheet" href="/fancybox/jquery.fancybox.css">

  
<script src="/fancybox/jquery.fancybox.pack.js"></script>




<script src="/js/script.js"></script>




  </div><!-- hexo-inject:begin --><!-- Begin: Injected MathJax -->
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({"tex2jax":{"inlineMath":[["$","$"],["\\(","\\)"]],"skipTags":["script","noscript","style","textarea","pre","code"],"processEscapes":true},"TeX":{"equationNumbers":{"autoNumber":"AMS"}},"extensions":["tex2jax.js"],"jax":["input/TeX","output/HTML-CSS"]});
</script>

<script type="text/x-mathjax-config">
  MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
      all[i].SourceElement().parentNode.className += ' has-jax';
    }
  });
</script>

<script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js">
</script>
<!-- End: Injected MathJax -->
<!-- hexo-inject:end -->
</body>
</html>